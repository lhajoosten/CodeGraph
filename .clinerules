# CodeGraph - Claude Code Instructions

## Project Context
CodeGraph is an AI coding agent platform that provides autonomous software development capabilities. The platform uses multiple Claude models orchestrated through LangGraph to handle planning, coding, testing, and code review tasks.

### Architecture Overview
- **Frontend**: React 18 + TypeScript + Vite + Tailwind CSS + Shadcn/ui
- **Backend**: Python 3.11+ + FastAPI + LangGraph + SQLAlchemy 2.0 (async)
- **AI**: Anthropic Claude (Haiku 4.5, Sonnet 4.5, Opus 4)
- **Database**: PostgreSQL 16 with pgvector extension
- **Caching**: Redis for task queues and caching
- **Monitoring**: LangSmith for agent tracing and observability

### Model Usage Strategy
- **Haiku 4.5**: Chat interface, simple queries, quick responses
- **Sonnet 4.5**: Planning agent, simple coding tasks, testing agent, code review
- **Opus 4**: Complex coding tasks, architectural changes, algorithm implementation

## Monorepo Structure
```
codegraph/
├── apps/
│   ├── backend/              # FastAPI application
│   └── frontend/             # React application
├── packages/
│   ├── types/                # Shared TypeScript/Python types
│   └── contracts/            # API contracts (OpenAPI/Pydantic)
├── docker/                   # Docker configurations
├── docs/                     # Documentation
└── scripts/                  # Utility scripts
```

## Global Coding Standards

### Version Control
- **Conventional Commits**: Use format `type(scope): description`
  - Types: `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`, `perf`
  - Example: `feat(agents): add code review agent with security checks`
- **Branch Naming**: `feature/*`, `bugfix/*`, `hotfix/*`, `docs/*`
- **No Direct Commits**: Always use PRs to main branch
- **Squash Commits**: Keep history clean with meaningful commit messages

### Code Quality
- **Type Safety**: Use type hints (Python) and strict TypeScript everywhere
- **Self-Documenting Code**: Clear variable/function names, avoid abbreviations
- **Comments**: Only for complex logic or "why" not "what"
- **DRY Principle**: Don't Repeat Yourself - extract common patterns
- **SOLID Principles**: Follow object-oriented design principles

### Documentation
- **Docstrings**: Every public function/class (Python: Google style, TypeScript: JSDoc)
- **README Files**: Each major directory should have a README explaining its purpose
- **API Documentation**: Auto-generated via FastAPI and maintained in OpenAPI spec
- **Architecture Docs**: Keep docs/architecture/ up to date with system changes

### Testing
- **Minimum Coverage**: 80% for both frontend and backend
- **Test Pyramid**: More unit tests, fewer integration tests, minimal e2e tests
- **Test Files**: Co-locate with source (`__tests__/` or `*.test.ts` pattern)
- **Mocking**: Mock external dependencies (APIs, databases, file system)
- **Test Data**: Use factories/fixtures, never hardcode test data

### Security
- **No Secrets in Code**: Use environment variables for all sensitive data
- **Input Validation**: Validate all user inputs (Pydantic for backend, Zod for frontend)
- **SQL Injection**: Use parameterized queries (SQLAlchemy handles this)
- **XSS Prevention**: React handles by default, be careful with dangerouslySetInnerHTML
- **Authentication**: JWT tokens with proper expiration and refresh logic
- **Authorization**: Role-based access control (RBAC) for all endpoints

### Error Handling
- **Structured Logging**: Use structlog (Python) and console context (TypeScript)
- **Error Context**: Always include relevant context (user_id, task_id, etc.)
- **Fail Fast**: Catch errors early, don't let them propagate silently
- **User-Friendly Messages**: Never expose internal errors to users
- **Exception Hierarchy**: Custom exceptions that extend base exceptions

### Performance
- **Async/Await**: Use async operations for all I/O (database, API calls, file system)
- **Connection Pooling**: Properly configured for database and Redis
- **Caching**: Use Redis for frequently accessed data
- **Lazy Loading**: Load data only when needed (React lazy loading, database select queries)
- **Pagination**: Always paginate large datasets (default: 50 items per page)

## AI Agent Development

### Agent Architecture
- **LangGraph Workflows**: All agents implemented as LangGraph state machines
- **State Management**: Use TypedDict for agent state definitions
- **Tool Usage**: Each agent has specific tools (file ops, code execution, GitHub API)
- **Human-in-the-Loop**: Checkpoints for critical operations (file deletion, schema changes)
- **Error Recovery**: Implement retry logic with exponential backoff

### LangSmith Integration
- **Trace Everything**: Every agent execution must be traceable
- **Include Context**: Add relevant metadata (task_id, user_id, model used)
- **Token Tracking**: Log token usage for cost monitoring
- **Performance Metrics**: Track execution time per agent/tool

### Cost Optimization (CRITICAL - Hard Constraints)

#### Model Tiering (Golden Rule)
- **Haiku 4.5**: Routing, classification, planning (max 200 tokens), tool selection, code diffs, exploration
- **Sonnet 4.5**: Final synthesis/polish ONLY (max 800 tokens), user-visible answers
- **Opus 4**: Reserved for complex architectural decisions (use sparingly, NOT as default)

**Pattern**:
```
User Input → Haiku (intent) → Haiku (plan) → Haiku (diffs/tools) → Sonnet (final polish, max 800 tokens)
```

**Rule**: If Sonnet appears anywhere except final synthesis, cost budget is exceeded.

#### Hard Output Token Caps
- **Sonnet max_output_tokens = 800** (Non-negotiable)
- **No chain-of-thought reasoning** in responses
- **No explaining your reasoning** to users
- **No intermediate step exposure**
- **Respond concisely**: Never restate the problem or show debug logs

#### Structured Outputs Over Explanations
- BAD: "Explain why this code works..."
- GOOD: `{"change_summary": "...", "diff": "...", "risk_level": "low"}`
- Use JSON/structured formats, never verbose prose for internal reasoning

#### Cache Discipline
- **CACHE ONLY immutable content**: Framework docs, static schemas, API contracts, coding standards
- **NEVER cache**: Conversation history, partial outputs, planning steps, tool results
- Rule of Thumb: If it changes per request → do not cache

#### Early-Exit Guards
- Use cheap Haiku pre-checks before expensive flows
- Stop if: "No code change needed", "User wants explanation only", "Duplicate request"
- Pattern: `Haiku → should_continue: true|false` (abort early = zero Sonnet cost)

#### Diff-Only Code Generation
- Never generate full files
- Use unified diffs only
- Context window ≤ 200 lines
- One file at a time
- This alone cuts output tokens by 10×

#### LangSmith Cost Monitoring
- Set cost per run limits
- Alert on token budget overages
- Detect model misuse (Sonnet outside final node)
- Fail run if Sonnet output tokens exceed 2k per trace

#### Expected Savings
- Cap Sonnet output: 70-85% reduction
- Model tiering: 40-60% reduction
- Cache hygiene: 20-30% reduction
- Early exits: Variable but large
- Diff-only code: 5-10× per code task
- **Combined target**: Reduce costs by 85-90%

## Dependency Management

### Backend (Python)
- **Poetry**: Use for all dependency management
- **Version Pinning**: Pin exact versions in production, use `^` for development
- **Security Audits**: Run `poetry audit` weekly
- **Minimal Dependencies**: Prefer stdlib when possible

### Frontend (TypeScript)
- **npm/pnpm**: Use pnpm for faster installs and better disk usage
- **Version Pinning**: Use exact versions in package.json
- **Bundle Size**: Monitor bundle size, lazy load heavy dependencies
- **Tree Shaking**: Ensure dependencies are tree-shakeable

## CI/CD

### GitHub Actions Workflows
- **On Every PR**: Linting, type checking, tests, build verification
- **On Main Merge**: Deploy to staging environment
- **Manual Deploy**: Production deployment requires manual approval
- **Docker Build**: Build and push Docker images on main branch

### Quality Gates
- **Tests Must Pass**: 100% required for merge
- **Coverage**: Maintain or increase coverage
- **Linting**: Zero linting errors allowed
- **Type Checking**: Zero type errors allowed
- **Build**: Must build successfully

## Development Workflow

### Local Development
1. **Start Services**: `docker-compose up -d postgres redis`
2. **Backend**: `cd apps/backend && poetry run uvicorn src.main:app --reload`
3. **Frontend**: `cd apps/frontend && npm run dev`
4. **Tests**: Run tests before committing

### Making Changes
1. **Create Branch**: `git checkout -b feature/your-feature`
2. **Make Changes**: Follow coding standards
3. **Write Tests**: Test your changes
4. **Update Docs**: If changing APIs or architecture
5. **Commit**: Use conventional commits
6. **Push**: `git push origin feature/your-feature`
7. **Create PR**: Use PR template, add relevant labels

### Code Review
- **Review Checklist**: Tests, documentation, security, performance
- **Be Kind**: Constructive feedback only
- **Approve**: Require at least one approval (for team projects)
- **Merge**: Squash and merge to keep history clean

## Common Patterns

### API Endpoint Pattern (Backend)
```python
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from src.core.deps import get_db, get_current_user
from src.models import Task, User
from src.schemas import TaskCreate, TaskResponse
from src.services import task_service

router = APIRouter(prefix="/tasks", tags=["tasks"])

@router.post("", response_model=TaskResponse, status_code=status.HTTP_201_CREATED)
async def create_task(
    task_data: TaskCreate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> Task:
    """Create a new coding task.

    Args:
        task_data: Task creation data
        db: Database session
        current_user: Authenticated user

    Returns:
        Created task instance

    Raises:
        HTTPException: If repository not found or user lacks permission
    """
    return await task_service.create_task(db, task_data, current_user.id)
```

### React Component Pattern (Frontend)
```typescript
import { useState } from 'react';
import { useQuery, useMutation } from '@tanstack/react-query';
import { Button } from '@/components/ui/button';
import { TaskStatus } from '@/types';
import { fetchTask, updateTask } from '@/services/api';

interface TaskDetailProps {
  taskId: number;
  onUpdate?: (task: Task) => void;
}

export function TaskDetail({ taskId, onUpdate }: TaskDetailProps) {
  const [isEditing, setIsEditing] = useState(false);

  const { data: task, isLoading, error } = useQuery({
    queryKey: ['task', taskId],
    queryFn: () => fetchTask(taskId),
  });

  const updateMutation = useMutation({
    mutationFn: updateTask,
    onSuccess: (updatedTask) => {
      onUpdate?.(updatedTask);
      setIsEditing(false);
    },
  });

  if (isLoading) return Loading...;
  if (error) return Error: {error.message};
  if (!task) return null;

  return (

      {task.title}
      {task.description}

      {isEditing ? (
        <Button onClick={() => updateMutation.mutate(task)}>
          Save Changes

      ) : (
        <Button onClick={() => setIsEditing(true)}>
          Edit Task

      )}

  );
}
```

### LangGraph Agent Pattern
```python
from typing import TypedDict, List
from langgraph.graph import StateGraph, END
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage

class AgentState(TypedDict):
    """State shared across agent nodes."""
    task_id: int
    description: str
    plan: List[str]
    code_changes: List[dict]
    test_results: dict
    messages: List[HumanMessage | AIMessage]
    current_step: int
    error: str | None

def planning_node(state: AgentState) -> AgentState:
    """Break down task into actionable steps."""
    llm = ChatAnthropic(model="claude-sonnet-4-20250514")

    response = llm.invoke([
        HumanMessage(content=f"Break down this task: {state['description']}")
    ])

    plan = extract_plan_from_response(response.content)

    return {
        **state,
        "plan": plan,
        "current_step": 0,
        "messages": state["messages"] + [response]
    }

def should_continue(state: AgentState) -> str:
    """Determine next node in workflow."""
    if state.get("error"):
        return "error_handler"
    if state["current_step"] >= len(state["plan"]):
        return END
    return "coding_node"

# Build workflow
workflow = StateGraph(AgentState)
workflow.add_node("planning", planning_node)
workflow.add_node("coding", coding_node)
workflow.add_node("testing", testing_node)
workflow.add_node("error_handler", error_handler_node)

workflow.set_entry_point("planning")
workflow.add_conditional_edges("planning", should_continue)
workflow.add_edge("coding", "testing")
workflow.add_conditional_edges("testing", should_continue)

agent = workflow.compile()
```

## When to Ask for Clarification

Claude Code should ask for clarification when:
- **Ambiguous Requirements**: Task description lacks necessary details
- **Breaking Changes**: Change would break existing functionality
- **Security Concerns**: Operation might introduce security vulnerabilities
- **Data Loss Risk**: Operation could result in data loss
- **Architecture Decision**: Major architectural change needed
- **Multiple Valid Approaches**: Several equally valid implementation paths

## Project-Specific Context

### This is NOT a CRUD app
CodeGraph is a sophisticated AI agent platform. Focus on:
- **Agent orchestration** and workflow design
- **LLM integration** and prompt engineering
- **Real-time streaming** of agent execution
- **Code analysis** and generation
- **GitHub integration** for repository operations

### Key Technical Challenges
- **Streaming LLM responses** to frontend in real-time
- **Managing agent state** across long-running tasks
- **Cost optimization** by selecting appropriate models
- **Error handling** in multi-step agent workflows
- **Security** when executing generated code

### Success Criteria
- Agents can complete coding tasks autonomously
- Full traceability via LangSmith
- <500ms API response time for non-agent endpoints
- >80% test coverage
- Zero security vulnerabilities
- Clean, maintainable code that follows standards

## Remember

- **This is a learning project**: Document decisions and rationale
- **Quality over speed**: Take time to do things right
- **Security first**: Never compromise on security
- **User experience matters**: Even for developer tools
- **Test everything**: Especially agent behavior and LLM integration
- **Keep it simple**: Avoid over-engineering
- **Document as you go**: Future you will thank present you